{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Importando as bibliotecas necessárias\n",
    "\n",
    "Nesse notebook as bibliotecas utilizadas serão:\n",
    "\n",
    "* Tensorflow - Para criação e treinamento das redes neurais e outras funções de pré-processamento de dados\n",
    "* Numpy - Para funções matemáticas\n",
    "\n",
    "E outras bibliotecas nativas do Python - time e os"
   ],
   "id": "dd92736f8b1f5860"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "arquivo_path = \"machado.txt\"",
   "id": "51e792f9154c1a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = open(arquivo_path, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'O texto contém: {len(text)} caracteres')\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} caracteres únicos')"
   ],
   "id": "5e72d3f183f64894",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pré-processando os dados\n",
    "\n",
    "Para a nossa rede, cada caracter precisa ser convertido em um valor numérico para identificação, uma vez que as redes neurais só trabalham com números."
   ],
   "id": "c981e07eb315571e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "caracteres_para_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "ids_para_caracteres = tf.keras.layers.StringLookup(\n",
    "    vocabulary=caracteres_para_ids.get_vocabulary(), invert=True, mask_token=None)\n"
   ],
   "id": "f3210605cde6faf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Função para converter de volta os IDs em textos\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(caracteres_para_ids(ids), axis=-1)"
   ],
   "id": "2f011f0bdeb1fe68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dividindo o texto em sequencias de entrada\n",
    "\n",
    "Em seguida, precisamos dividir o texto em sequencias menores de entrada para que a nossa rede possa receber no treinamento\n"
   ],
   "id": "19214bf3d2db19e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "texto_em_ids = caracteres_para_ids(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "dataset_de_ids = tf.data.Dataset.from_tensor_slices(texto_em_ids)"
   ],
   "id": "903b6bd66544a0c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tamanho_da_sequencia_em_caracteres = 100\n",
    "dados_por_dataset = len(text) // (tamanho_da_sequencia_em_caracteres + 1)"
   ],
   "id": "364eda10d720322a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequencias = dataset_de_ids.batch(tamanho_da_sequencia_em_caracteres+1, drop_remainder=True)",
   "id": "dd49183b8c10964f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def divide_input(sequencia):\n",
    "    input_text = sequencia[:-1]\n",
    "    target_text = sequencia[1:]\n",
    "    return input_text, target_text"
   ],
   "id": "89c3b37f33f60b17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = sequencias.map(divide_input)",
   "id": "2a996385d35af1be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Criando os lotes de treinamento\n",
    "Utilizando funções do Tensorflow, vamos converter os dados em um formato de entrada válido para a nossa rede neural"
   ],
   "id": "6b62013b9a718144"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TAMANHO_BATCH = 64\n",
    "\n",
    "# Tamanho do buffer para embaralhar os exemplos do dataset\n",
    "TAMANHO_BUFFER = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(TAMANHO_BUFFER)\n",
    "    .batch(TAMANHO_BATCH, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ],
   "id": "4c2d8416874c850f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Construindo o modelo\n",
    "Utilizando o `keras.model`, vamos criar uma rede neural recorrente com duas camadas LSTM (Long short-term memory).\n",
    "No artigo original, o autor cria um modelo muito semelhante com uma única camada GRU (Gated Recurrent Unit), que são mais baratas computacionalmente porém possuem uma menor eficiencia devido a sua arquitetura com 2 portões (atualização e redefinição), ao contrário da LSTM que possui 3 portões (entrada, esquecimento e saída).\n",
    "\n",
    "A arquitetura da rede é bem simples:\n",
    "- Uma camada embedding com a dimensão de 256\n",
    "- Duas camadas LSTM com 2048 neuronios\n",
    "- Uma camada dense com o mesmo tamanho que a quantidade de caracteres, onde cada neurônio representa um caracter\n",
    "\n",
    "O uso de duas camadas LSTM garante que o modelo aprenda padrões de maneira eficiente e com maior precisão"
   ],
   "id": "ca4fb645a47eb479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dimensao_de_embedding = 256\n",
    "neuronios_lstm = 2048"
   ],
   "id": "6a5a2e605a28386b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ModeloDeLinguagem(tf.keras.Model):\n",
    "  def __init__(self, tamanho_do_vocabulario, dimensao_de_embedding, neuronios_lstm):\n",
    "    super().__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(tamanho_do_vocabulario, dimensao_de_embedding)\n",
    "    self.lstm1 = tf.keras.layers.LSTM(neuronios_lstm,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True)\n",
    "    self.lstm2 = tf.keras.layers.LSTM(neuronios_lstm,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(tamanho_do_vocabulario)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = self.embedding(inputs, training=training)\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    if states is None:\n",
    "        h0_1 = tf.zeros((batch_size, self.lstm1.units))\n",
    "        c0_1 = tf.zeros((batch_size, self.lstm1.units))\n",
    "        x, h1, c1 = self.lstm1(x, initial_state=[h0_1, c0_1], training=training)\n",
    "\n",
    "        h0_2 = tf.zeros((batch_size, self.lstm2.units))\n",
    "        c0_2 = tf.zeros((batch_size, self.lstm2.units))\n",
    "        x, h2, c2 = self.lstm2(x, initial_state=[h0_2, c0_2], training=training)\n",
    "    else:\n",
    "        (h1, c1), (h2, c2) = states\n",
    "        x, h1, c1 = self.lstm1(x, initial_state=[h1, c1], training=training)\n",
    "        x, h2, c2 = self.lstm2(x, initial_state=[h2, c2], training=training)\n",
    "\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "        return x, [(h1, c1), (h2, c2)]\n",
    "    else:\n",
    "        return x"
   ],
   "id": "1a6de87988446644",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ModeloDeLinguagem(\n",
    "    tamanho_do_vocabulario=len(caracteres_para_ids.get_vocabulary()),\n",
    "    dimensao_de_embedding=dimensao_de_embedding,\n",
    "    neuronios_lstm=neuronios_lstm)"
   ],
   "id": "f7cb30a02386f9cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Treinando o modelo!!!!\n",
   "id": "c3ca15cbb43921d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ],
   "id": "e65c8981fbd4266c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Salva os pesos do modelo em checkpoints do treinamento\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ],
   "id": "ae7644f299ebaf77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "EPOCHS = 30",
   "id": "c452eeee2029420f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])",
   "id": "f2cee5b046806a10",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
