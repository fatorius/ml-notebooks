{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias\n",
    "\n",
    "Nesse notebook as bibliotecas utilizadas serão:\n",
    "\n",
    "* sklearn - Para geração do dataset de treino\n",
    "* Tensorflow - Para criação e manipulação das redes neurais utilizadas\n",
    "* Pandas - Para lidar com os arquivos .csv que compoem o dataset\n",
    "* Numpy - Para funções matemáticas - nesse caso: a random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation, BatchNormalization\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abrindo o dataset\n",
    "\n",
    "O dataset utilizado foi obtido no formato de .csv no [Kaggle](https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format).\n",
    "Ele será aberto utilizando o Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path = '/kaggle/input/az-handwritten-alphabets-in-csv-format/A_Z Handwritten Data.csv'\n",
    "\n",
    "names = ['class']\n",
    "for id in range(1,785):\n",
    "    names.append(id)\n",
    "\n",
    "df = pd.read_csv(file_path,header=None, names=names)\n",
    "\n",
    "class_mapping = {}\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "for i in range(len(alphabets)):\n",
    "    class_mapping[i] = alphabets[i]\n",
    "    \n",
    "df['class'].map(class_mapping).unique()\n",
    "\n",
    "y_full = df.pop('class')\n",
    "x_full = df.to_numpy().reshape(-1,28,28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split do dataset\n",
    "\n",
    "Os samples presentes no dataframe serão divididos em 2 conjuntos: o de treino e o de teste.\n",
    "\n",
    "Os samples presentes no dataset de testes não serão apresentados durante o treinamento da rede neural para que a sua precisão possa ser validada com dados inéditos - simulando o ambiente real de produção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=3,test_size=val_split)\n",
    "\n",
    "for train_ids, test_ids in splitter.split(x_full, y_full):\n",
    "    X_train_full, y_train_full = x_full[train_ids], y_full[train_ids].to_numpy()\n",
    "    X_test, y_test = x_full[test_ids], y_full[test_ids].to_numpy()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processando as imagens\n",
    "\n",
    "Para facilitar o aprendizado do modelo, as imagens serão pre-processadas.\n",
    "O pré-processamento consiste na aplicação do filtro de threshold, binarizando os valores, em outras palavras:\n",
    "\n",
    "Se o valor do pixel > 125, pixel = 1\n",
    "Se o valor do pixel < 125, pixel = 0\n",
    "\n",
    "Isso acontece pois os valores dos pixels em escala de cinza (entre 0 e 255) podem confudir a rede durante seu aprendizado, portanto as imagens são \"simplificadas\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagens antes do pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for i in range(1, 11):\n",
    "    \n",
    "    id = np.random.randint(len(X_train))\n",
    "    image, label = tf.squeeze(X_train[id]), class_mapping[int(y_train[id])]\n",
    "    \n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = (X_train > 125)\n",
    "X_test = (X_test > 125) \n",
    "X_valid = (X_valid > 125) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagens após o pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for i in range(1, 11):\n",
    "    \n",
    "    id = np.random.randint(len(X_train))\n",
    "    image, label = tf.squeeze(X_train[id]), class_mapping[int(y_train[id])]\n",
    "    \n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumento de dados\n",
    "\n",
    "Para o treino, também será usado uma técnica de aumento de dados, onde as imagens serão aleatoriamente rotacionadas em 10 graus para ambas as direções, além de ampliadas, movidas e diminuidas em 10%.\n",
    "\n",
    "Ao proporcionar mais dados para treino, essa técnica permite com que o modelo aprenda em cima de dados irregulares e diferentes dos previamente vistos, assim aumentando seu reconhecimento dos padrões dos caracteres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center = False,\n",
    "        samplewise_center = False,\n",
    "        featurewise_std_normalization = False,\n",
    "        samplewise_std_normalization = False,\n",
    "        zca_whitening = False,\n",
    "        rotation_range = 10,\n",
    "        zoom_range = 0.1, \n",
    "        width_shift_range = 0.1,  \n",
    "        height_shift_range = 0.1, \n",
    "        horizontal_flip = False,  \n",
    "        vertical_flip = False,\n",
    "        validation_split=val_split\n",
    ") \n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitetura do modelo\n",
    "\n",
    "O modelo de redes neurais é composto por camadas convolucionais. \n",
    "Em algumas camadas foi utilizado a Regularização L2 para evitar o overfit.\n",
    "Outras tecnicas para evitar o overfit também foram utilizadas, como camadas Dropout e o callback de ReduceLROnPlateau, que reduz a taxa de aprendizado da rede caso esta fique estagnada.\n",
    "\n",
    "Também existem camadas de BatchNormalization, responsáveis por normalizar as ativações das camadas anteriores, permitindo com que as camadas aprendam de forma mais indenpendente e não sejam tão influenciadas pelas ativações das camadas anteriores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters = 32, kernel_size = 5, strides = 1, activation = 'relu', input_shape = (28,28,1), kernel_regularizer=l2(0.0005), name = 'convolution_1'),\n",
    "    Conv2D(filters = 32, kernel_size = 5, strides = 1, name = 'convolution_2', use_bias=False),\n",
    "    \n",
    "    BatchNormalization(name = 'batchnorm_1'),\n",
    "        \n",
    "    Activation(\"relu\"),\n",
    "    MaxPooling2D(pool_size = 2, strides = 2, name = 'max_pool_1'),\n",
    "    Dropout(0.25, name = 'dropout_1'),\n",
    "        \n",
    "    Conv2D(filters = 64, kernel_size = 3, strides = 1, activation = 'relu', kernel_regularizer=l2(0.0005), name = 'convolution_3'),\n",
    "        \n",
    "    Conv2D(filters = 64, kernel_size = 3, strides = 1, name = 'convolution_4', use_bias=False),\n",
    "        \n",
    "    BatchNormalization(name = 'batchnorm_2'),\n",
    "        \n",
    "    Activation(\"relu\"),\n",
    "    MaxPooling2D(pool_size = 2, strides = 2, name = 'max_pool_2'),\n",
    "    \n",
    "    Dropout(0.25, name = 'dropout_2'),\n",
    "    Flatten(name = 'flatten'),\n",
    "        \n",
    "    Dense(units = 256, name = 'fully_connected_1', use_bias=False),\n",
    "        \n",
    "    BatchNormalization(name = 'batchnorm_3'),\n",
    "    \n",
    "    Activation(\"relu\"),\n",
    "        \n",
    "    Dense(units = 128, name = 'fully_connected_2', use_bias=False),\n",
    "        \n",
    "    BatchNormalization(name = 'batchnorm_4'),\n",
    "        \n",
    "    Activation(\"relu\"),\n",
    "    \n",
    "    Dense(units = 84, name = 'fully_connected_3', use_bias=False),\n",
    "        \n",
    "    BatchNormalization(name = 'batchnorm_5'),\n",
    "        \n",
    "    Activation(\"relu\"),\n",
    "    \n",
    "    Dropout(0.25, name = 'dropout_3'),\n",
    "    \n",
    "    Dense(26, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "     loss='sparse_categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [\n",
    "    EarlyStopping(patience=3, restore_best_weights=True), \n",
    "    ModelCheckpoint(\"Model-v1.h5\", save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "     datagen.flow(X_train, y_train, batch_size=64,subset='training'),\n",
    "     validation_data=(X_valid, y_valid),\n",
    "     epochs=50,\n",
    "     callbacks=cbs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes\n",
    "\n",
    "O conjunto de testes foi composto por 2328 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test) #[0.035956788808107376, 0.9918245077133179]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o treinamento, a rede teve um desempenho de 99.18% de precisão no conjunto de testes "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
